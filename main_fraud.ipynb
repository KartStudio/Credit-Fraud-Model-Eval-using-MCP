{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3689760c-41f8-4a33-9c96-3fd17803950e",
    "_uuid": "3e0ad409d438c7c68ea6a76700a1e964a357453f"
   },
   "source": [
    "<h1 align=\"center\"> Credit Fraud Detector </h1>\n",
    "\n",
    "**Note:** There are still aspects of this kernel that will be subjected to changes. I've noticed a recent increase of interest towards this kernel so I will focus more on the steps I took and why I took them to make it clear why I took those steps.\n",
    "\n",
    "<h2>Before we Begin:  </h2>\n",
    "\n",
    "\n",
    "<h2> Introduction </h2>\n",
    "In this kernel we will use various predictive models to see how accurate they  are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n",
    "\n",
    "\n",
    "<h2> Our Goals: </h2>\n",
    "<ul>\n",
    "<li> Understand the little distribution of the \"little\" data that was provided to us. </li>\n",
    "<li> Create a 50/50 sub-dataframe ratio of \"Fraud\" and \"Non-Fraud\" transactions. (NearMiss Algorithm) </li>\n",
    "<li> Determine the Classifiers we are going to use and decide which one has a higher accuracy. </li>\n",
    "<li>Create a Neural Network and compare the accuracy to our best classifier. </li>\n",
    "<li>Understand common mistaked made with imbalanced datasets. </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h2> Outline: </h2>\n",
    "I. <b>Understanding our data</b><br>\n",
    "a) [Gather Sense of our data](#gather)<br><br>\n",
    "\n",
    "II. <b>Preprocessing</b><br>\n",
    "a) [Scaling and Distributing](#distributing)<br>\n",
    "b) [Splitting the Data](#splitting)<br><br>\n",
    "\n",
    "III. <b>Random UnderSampling and Oversampling</b><br>\n",
    "a) [Distributing and Correlating](#correlating)<br>\n",
    "b) [Anomaly Detection](#anomaly)<br>\n",
    "c) [Dimensionality Reduction and Clustering (t-SNE)](#clustering)<br>\n",
    "d) [Classifiers](#classifiers)<br>\n",
    "e) [A Deeper Look into Logistic Regression](#logistic)<br>\n",
    "f) [Oversampling with SMOTE](#smote)<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<h2>Correcting Previous Mistakes from Imbalanced Datasets: </h2>\n",
    "<ul>\n",
    "<li> Never test on the oversampled or undersampled dataset.</li>\n",
    "<li>If we want to implement cross validation, remember to oversample or undersample your training data <b>during</b> cross-validation, not before! </li>\n",
    "<li> Don't use <b>accuracy score </b> as a metric with imbalanced datasets (will be usually high and misleading), instead use <b>f1-score, precision/recall score or confusion matrix </b></li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<h2> References: </h2>\n",
    "<ul> \n",
    "<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aurélien Géron (O'Reilly). CopyRight 2017 Aurélien Géron  </li>\n",
    "<li><a src=\"https://www.youtube.com/watch?v=DQC_YE3I5ig&t=794s\" > Machine Learning - Over-& Undersampling - Python/ Scikit/ Scikit-Imblearn </a>by Coding-Maniac</li>\n",
    "<li><a src=\"https://www.kaggle.com/lane203j/auprc-5-fold-c-v-and-resampling-methods\"> auprc, 5-fold c-v, and resampling methods\n",
    "</a> by Jeremy Lane (Kaggle Notebook) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae8dd7f3-80a7-4db9-a132-823b0e48c041",
    "_uuid": "c999e5f1ac81513263d83883008f2844209e9e07"
   },
   "source": [
    "## Gather Sense of Our Data:\n",
    "<a id=\"gather\"></a>\n",
    "The first thing we must do is gather a <b> basic sense </b> of our data. Remember, except for the <b>transaction</b> and <b>amount</b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.   \n",
    "\n",
    "<h3> Summary: </h3>\n",
    "<ul>\n",
    "<li>The transaction amount is relatively <b>small</b>. The mean of all the mounts made is approximately USD 88. </li>\n",
    "<li>There are no <b>\"Null\"</b> values, so we don't have to work on ways to replace values. </li>\n",
    "<li> Most of the transactions were <b>Non-Fraud</b> (99.83%) of the time, while <b>Fraud</b> transactions occurs (017%) of the time in the dataframe. </li>\n",
    "</ul>\n",
    "\n",
    "<h3> Feature Technicalities: </h3>\n",
    "<ul>\n",
    "<li> <b>PCA Transformation: </b>  The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).</li>\n",
    "<li> <b>Scaling:</b> Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "# Imported Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "#Adding MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn # Essential for logging scikit-learn models\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#df = pd.read_csv('transactions.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///mnt/c/Users/Vimal%20Karthik/mcpservers/mcpcrit/mlruns/616483229949026516', creation_time=1750254481257, experiment_id='616483229949026516', last_update_time=1750254481257, lifecycle_stage='active', name='Credit Fraud Detection - Undersampling Strategy', tags={'mlflow.sharedViewState.d786a127d3139de6e5400a3561ed09dd4f9c3edda304234b356b49804bd1813b': 'deflate;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'}>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLflow Global Setup\n",
    "\n",
    "mlflow.set_experiment(\"Credit Fraud Detection - Undersampling Strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107536</th>\n",
       "      <td>70483</td>\n",
       "      <td>0.967967</td>\n",
       "      <td>-0.341611</td>\n",
       "      <td>1.288260</td>\n",
       "      <td>1.685099</td>\n",
       "      <td>-0.840565</td>\n",
       "      <td>0.794535</td>\n",
       "      <td>-0.639678</td>\n",
       "      <td>0.447705</td>\n",
       "      <td>1.075974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.341123</td>\n",
       "      <td>-0.637864</td>\n",
       "      <td>0.080202</td>\n",
       "      <td>0.202023</td>\n",
       "      <td>0.335258</td>\n",
       "      <td>-0.539935</td>\n",
       "      <td>0.078210</td>\n",
       "      <td>0.023935</td>\n",
       "      <td>38.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107537</th>\n",
       "      <td>70483</td>\n",
       "      <td>1.411721</td>\n",
       "      <td>-1.124226</td>\n",
       "      <td>0.674365</td>\n",
       "      <td>-1.298339</td>\n",
       "      <td>-1.752271</td>\n",
       "      <td>-0.903168</td>\n",
       "      <td>-1.003696</td>\n",
       "      <td>-0.198635</td>\n",
       "      <td>-1.768822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338540</td>\n",
       "      <td>-0.721368</td>\n",
       "      <td>0.136130</td>\n",
       "      <td>0.333874</td>\n",
       "      <td>0.119134</td>\n",
       "      <td>-0.415353</td>\n",
       "      <td>0.035291</td>\n",
       "      <td>0.038273</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107538</th>\n",
       "      <td>70484</td>\n",
       "      <td>-0.511283</td>\n",
       "      <td>1.188627</td>\n",
       "      <td>0.124054</td>\n",
       "      <td>1.125798</td>\n",
       "      <td>0.269740</td>\n",
       "      <td>0.262457</td>\n",
       "      <td>0.476244</td>\n",
       "      <td>0.501720</td>\n",
       "      <td>-0.454584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108216</td>\n",
       "      <td>0.447021</td>\n",
       "      <td>-0.036599</td>\n",
       "      <td>-0.768496</td>\n",
       "      <td>-0.357004</td>\n",
       "      <td>-0.177356</td>\n",
       "      <td>0.338919</td>\n",
       "      <td>0.187756</td>\n",
       "      <td>45.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107539</th>\n",
       "      <td>70484</td>\n",
       "      <td>-0.585447</td>\n",
       "      <td>0.700403</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>1.038926</td>\n",
       "      <td>-0.828735</td>\n",
       "      <td>-0.416937</td>\n",
       "      <td>0.728753</td>\n",
       "      <td>0.396668</td>\n",
       "      <td>-0.893780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318777</td>\n",
       "      <td>0.485238</td>\n",
       "      <td>0.329154</td>\n",
       "      <td>0.502393</td>\n",
       "      <td>-0.159137</td>\n",
       "      <td>-0.359822</td>\n",
       "      <td>-0.106756</td>\n",
       "      <td>-0.017112</td>\n",
       "      <td>162.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107540</th>\n",
       "      <td>70484</td>\n",
       "      <td>-0.704158</td>\n",
       "      <td>1.024871</td>\n",
       "      <td>0.510468</td>\n",
       "      <td>1.440932</td>\n",
       "      <td>-0.496481</td>\n",
       "      <td>-0.290660</td>\n",
       "      <td>0.723218</td>\n",
       "      <td>0.415662</td>\n",
       "      <td>-0.538652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222649</td>\n",
       "      <td>0.643436</td>\n",
       "      <td>0.268426</td>\n",
       "      <td>0.399180</td>\n",
       "      <td>-0.489387</td>\n",
       "      <td>-0.251839</td>\n",
       "      <td>0.315885</td>\n",
       "      <td>0.204278</td>\n",
       "      <td>112.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107541 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0           0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1           0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2           1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3           1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4           2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "107536  70483  0.967967 -0.341611  1.288260  1.685099 -0.840565  0.794535   \n",
       "107537  70483  1.411721 -1.124226  0.674365 -1.298339 -1.752271 -0.903168   \n",
       "107538  70484 -0.511283  1.188627  0.124054  1.125798  0.269740  0.262457   \n",
       "107539  70484 -0.585447  0.700403  0.583518  1.038926 -0.828735 -0.416937   \n",
       "107540  70484 -0.704158  1.024871  0.510468  1.440932 -0.496481 -0.290660   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1      -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2       0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3       0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4       0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "107536 -0.639678  0.447705  1.075974  ... -0.341123 -0.637864  0.080202   \n",
       "107537 -1.003696 -0.198635 -1.768822  ... -0.338540 -0.721368  0.136130   \n",
       "107538  0.476244  0.501720 -0.454584  ...  0.108216  0.447021 -0.036599   \n",
       "107539  0.728753  0.396668 -0.893780  ...  0.318777  0.485238  0.329154   \n",
       "107540  0.723218  0.415662 -0.538652  ...  0.222649  0.643436  0.268426   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1      -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3      -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4       0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "107536  0.202023  0.335258 -0.539935  0.078210  0.023935   38.50      0  \n",
       "107537  0.333874  0.119134 -0.415353  0.035291  0.038273   60.00      0  \n",
       "107538 -0.768496 -0.357004 -0.177356  0.338919  0.187756   45.05      0  \n",
       "107539  0.502393 -0.159137 -0.359822 -0.106756 -0.017112  162.99      0  \n",
       "107540  0.399180 -0.489387 -0.251839  0.315885  0.204278  112.50      0  \n",
       "\n",
       "[107541 rows x 31 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('transactionscopy.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "_cell_guid": "376ce881-463a-4a09-9ac0-c63f85577eec",
    "_kg_hide-input": true,
    "_uuid": "93031e732e5aca3a2b4984799d6bf58d76e4b52d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "      <td>107541.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>44283.824978</td>\n",
       "      <td>-0.257245</td>\n",
       "      <td>-0.026633</td>\n",
       "      <td>0.682226</td>\n",
       "      <td>0.156946</td>\n",
       "      <td>-0.283815</td>\n",
       "      <td>0.097668</td>\n",
       "      <td>-0.116124</td>\n",
       "      <td>0.058905</td>\n",
       "      <td>-0.055178</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030629</td>\n",
       "      <td>-0.107244</td>\n",
       "      <td>-0.037206</td>\n",
       "      <td>0.010229</td>\n",
       "      <td>0.133463</td>\n",
       "      <td>0.025790</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>96.183166</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17765.989610</td>\n",
       "      <td>1.852083</td>\n",
       "      <td>1.646778</td>\n",
       "      <td>1.316015</td>\n",
       "      <td>1.343586</td>\n",
       "      <td>1.347879</td>\n",
       "      <td>1.299104</td>\n",
       "      <td>1.207157</td>\n",
       "      <td>1.230872</td>\n",
       "      <td>1.109664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740718</td>\n",
       "      <td>0.639468</td>\n",
       "      <td>0.623217</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>0.439825</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.391956</td>\n",
       "      <td>0.319729</td>\n",
       "      <td>260.882300</td>\n",
       "      <td>0.046794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-33.680984</td>\n",
       "      <td>-5.172595</td>\n",
       "      <td>-42.147898</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-31.764946</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-9.283925</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-2.534330</td>\n",
       "      <td>-9.390980</td>\n",
       "      <td>-9.617915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34295.000000</td>\n",
       "      <td>-1.023067</td>\n",
       "      <td>-0.595506</td>\n",
       "      <td>0.177025</td>\n",
       "      <td>-0.712031</td>\n",
       "      <td>-0.906954</td>\n",
       "      <td>-0.645800</td>\n",
       "      <td>-0.605454</td>\n",
       "      <td>-0.134689</td>\n",
       "      <td>-0.697448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224135</td>\n",
       "      <td>-0.533664</td>\n",
       "      <td>-0.176687</td>\n",
       "      <td>-0.323256</td>\n",
       "      <td>-0.130676</td>\n",
       "      <td>-0.323557</td>\n",
       "      <td>-0.061109</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>7.050000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46137.000000</td>\n",
       "      <td>-0.260348</td>\n",
       "      <td>0.077957</td>\n",
       "      <td>0.757746</td>\n",
       "      <td>0.184841</td>\n",
       "      <td>-0.318356</td>\n",
       "      <td>-0.155212</td>\n",
       "      <td>-0.071859</td>\n",
       "      <td>0.077599</td>\n",
       "      <td>-0.121597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056824</td>\n",
       "      <td>-0.083264</td>\n",
       "      <td>-0.049293</td>\n",
       "      <td>0.066145</td>\n",
       "      <td>0.171654</td>\n",
       "      <td>-0.068983</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.023366</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58587.000000</td>\n",
       "      <td>1.155052</td>\n",
       "      <td>0.738480</td>\n",
       "      <td>1.382691</td>\n",
       "      <td>1.024248</td>\n",
       "      <td>0.244617</td>\n",
       "      <td>0.492396</td>\n",
       "      <td>0.409532</td>\n",
       "      <td>0.369039</td>\n",
       "      <td>0.541522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120306</td>\n",
       "      <td>0.313628</td>\n",
       "      <td>0.080706</td>\n",
       "      <td>0.407326</td>\n",
       "      <td>0.421028</td>\n",
       "      <td>0.294259</td>\n",
       "      <td>0.084557</td>\n",
       "      <td>0.076628</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70484.000000</td>\n",
       "      <td>1.960497</td>\n",
       "      <td>18.902453</td>\n",
       "      <td>4.226108</td>\n",
       "      <td>16.715537</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>22.529298</td>\n",
       "      <td>36.677268</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>10.392889</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>19.002942</td>\n",
       "      <td>4.016342</td>\n",
       "      <td>5.541598</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>12.152401</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>19656.530000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  107541.000000  107541.000000  107541.000000  107541.000000   \n",
       "mean    44283.824978      -0.257245      -0.026633       0.682226   \n",
       "std     17765.989610       1.852083       1.646778       1.316015   \n",
       "min         0.000000     -56.407510     -72.715728     -33.680984   \n",
       "25%     34295.000000      -1.023067      -0.595506       0.177025   \n",
       "50%     46137.000000      -0.260348       0.077957       0.757746   \n",
       "75%     58587.000000       1.155052       0.738480       1.382691   \n",
       "max     70484.000000       1.960497      18.902453       4.226108   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  107541.000000  107541.000000  107541.000000  107541.000000   \n",
       "mean        0.156946      -0.283815       0.097668      -0.116124   \n",
       "std         1.343586       1.347879       1.299104       1.207157   \n",
       "min        -5.172595     -42.147898     -26.160506     -31.764946   \n",
       "25%        -0.712031      -0.906954      -0.645800      -0.605454   \n",
       "50%         0.184841      -0.318356      -0.155212      -0.071859   \n",
       "75%         1.024248       0.244617       0.492396       0.409532   \n",
       "max        16.715537      34.801666      22.529298      36.677268   \n",
       "\n",
       "                  V8             V9  ...            V21            V22  \\\n",
       "count  107541.000000  107541.000000  ...  107541.000000  107541.000000   \n",
       "mean        0.058905      -0.055178  ...      -0.030629      -0.107244   \n",
       "std         1.230872       1.109664  ...       0.740718       0.639468   \n",
       "min       -73.216718      -9.283925  ...     -34.830382     -10.933144   \n",
       "25%        -0.134689      -0.697448  ...      -0.224135      -0.533664   \n",
       "50%         0.077599      -0.121597  ...      -0.056824      -0.083264   \n",
       "75%         0.369039       0.541522  ...       0.120306       0.313628   \n",
       "max        20.007208      10.392889  ...      27.202839      10.503090   \n",
       "\n",
       "                 V23            V24            V25            V26  \\\n",
       "count  107541.000000  107541.000000  107541.000000  107541.000000   \n",
       "mean       -0.037206       0.010229       0.133463       0.025790   \n",
       "std         0.623217       0.595667       0.439825       0.491500   \n",
       "min       -44.807735      -2.836627     -10.295397      -2.534330   \n",
       "25%        -0.176687      -0.323256      -0.130676      -0.323557   \n",
       "50%        -0.049293       0.066145       0.171654      -0.068983   \n",
       "75%         0.080706       0.407326       0.421028       0.294259   \n",
       "max        19.002942       4.016342       5.541598       3.517346   \n",
       "\n",
       "                 V27            V28         Amount          Class  \n",
       "count  107541.000000  107541.000000  107541.000000  107541.000000  \n",
       "mean        0.001644       0.001607      96.183166       0.002195  \n",
       "std         0.391956       0.319729     260.882300       0.046794  \n",
       "min        -9.390980      -9.617915       0.000000       0.000000  \n",
       "25%        -0.061109      -0.005076       7.050000       0.000000  \n",
       "50%         0.010729       0.023366      25.150000       0.000000  \n",
       "75%         0.084557       0.076628      87.000000       0.000000  \n",
       "max        12.152401      33.847808   19656.530000       1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "_cell_guid": "03ddb929-5bc8-4af4-90cd-21dcbb57560d",
    "_kg_hide-input": true,
    "_uuid": "38bec67888aa534e9739e95ef9fac62d27a87021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good No Null Values!\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "_cell_guid": "6a526b6c-8463-4f6f-92b0-e8a3a21cbb2e",
    "_kg_hide-input": true,
    "_uuid": "479a5f12d3dd68262316a17b4b7b3499e0a2cbe0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "_cell_guid": "01c007fa-0fcc-4eea-84ff-0861a2f8c533",
    "_kg_hide-input": true,
    "_uuid": "f6b96ff34855e3bf7af1f6979342b01c473e4e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.78 % of the dataset\n",
      "Frauds 0.22 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "\n",
    "# --- Add these calculations here ---\n",
    "original_total_transactions = len(df)\n",
    "original_fraud_count = df['Class'].value_counts().get(1, 0) # Use .get to handle cases where class 1 might not exist\n",
    "original_non_fraud_count = df['Class'].value_counts().get(0, 0)\n",
    "\n",
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "558c9b60-3f52-4da5-92fa-9fc4acbdbb3a",
    "_uuid": "c2bb0945a312508e908386fc87adc227f0afe0e0"
   },
   "source": [
    "**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "_cell_guid": "657bc987-4b15-4cfa-b290-c39a2632e2ac",
    "_kg_hide-input": true,
    "_uuid": "337caaf6ed3f65beedb24a74eebb22d97ff52ba4"
   },
   "outputs": [],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot(x='Class', data=df, palette=colors)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
    "plt.savefig(\"original_class_distribution.png\")\n",
    "plt.close() # Close the figure to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
    "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421"
   },
   "source": [
    "**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "_cell_guid": "cee315f2-325f-42b6-a640-736f10c272cc",
    "_kg_hide-input": true,
    "_uuid": "cfa51792bf6f8a6b318ae1bffcff4e922b1d1917"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "72fdda5e-7f82-488d-a433-6157d6180bb8",
    "_uuid": "c5d6781e61c0ee84e72d26e8465bfd98ef91f3b9"
   },
   "source": [
    "<h2> Scaling and Distributing </h2>\n",
    "<a id=\"distributing\"></a>\n",
    "In this phase of our kernel, we will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "\n",
    "<h3> What is a sub-Sample?</h3>\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "\n",
    "<h3> Why do we create a sub-Sample?</h3>\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
    "<ul>\n",
    "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
    "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
    "</ul>\n",
    "\n",
    "<h3>Summary: </h3> \n",
    "<ul>\n",
    "<li> <b>Scaled amount </b> and <b> scaled time </b> are the columns with scaled values. </li>\n",
    "<li> There are <b>492 cases </b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. </li>\n",
    "<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. </b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "_cell_guid": "d5d64bf0-2fbb-4096-a265-f68887bf2fde",
    "_kg_hide-input": true,
    "_uuid": "1501ec379c9b5c39c3857ba0febd0aedee9c30d5"
   },
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "if 'Amount' in df.columns and 'Time' in df.columns: \n",
    "    df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "    df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "    df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "_cell_guid": "cdb9bb1e-9fab-4fd1-a409-468ba8bc36ee",
    "_kg_hide-input": true,
    "_uuid": "a33d701247ab45d849c5e94735346a738a6c6970"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.556848</td>\n",
       "      <td>-1.899267</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.280926</td>\n",
       "      <td>-1.899267</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.421639</td>\n",
       "      <td>-1.899226</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.230144</td>\n",
       "      <td>-1.899226</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.560851</td>\n",
       "      <td>-1.899185</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.556848    -1.899267 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.280926    -1.899267  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.421639    -1.899226 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.230144    -1.899226 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.560851    -1.899185 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# Amount and Time are Scaled!\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a59c8c8d-a4bc-4671-aa2f-9f959c142cde",
    "_uuid": "5119c4ea9e0b9031dbc5937b56323da224985024"
   },
   "source": [
    "### Splitting the Data (Original DataFrame)\n",
    "<a id=\"splitting\"></a>\n",
    "Before proceeding with the <b> Random UnderSampling technique</b> we have to separate the orginal dataframe. <b> Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "_cell_guid": "c6c962cc-6f38-4a00-bcd7-ce9d91db954c",
    "_kg_hide-input": true,
    "_uuid": "9f7b5d920703b3a3c8c0f62bc6042e4615bc8324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.78 % of the dataset\n",
      "Frauds 0.22 % of the dataset\n",
      "Train: [ 11343  11710  11841 ... 107538 107539 107540] Test: [    0     1     2 ... 21544 21545 21546]\n",
      "Train: [     0      1      2 ... 107538 107539 107540] Test: [11343 11710 11841 ... 43048 43049 43050]\n",
      "Train: [     0      1      2 ... 107538 107539 107540] Test: [30314 30384 30398 ... 64549 64550 64551]\n",
      "Train: [     0      1      2 ... 107538 107539 107540] Test: [45732 46909 46918 ... 86045 86046 86047]\n",
      "Train: [    0     1     2 ... 86045 86046 86047] Test: [ 77387  77682  79525 ... 107538 107539 107540]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.99780317 0.00219683]\n",
      "[0.99781477 0.00218523]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "956d34b9-e562-4b70-a2f8-fbe060273a83",
    "_uuid": "cc554c4ffec656cb38d01c034f2cd338e1cb4565"
   },
   "source": [
    "## Random Under-Sampling:\n",
    "<img src=\"http://contrib.scikit-learn.org/imbalanced-learn/stable/_images/sphx_glr_plot_random_under_sampler_001.png\">\n",
    "\n",
    "In this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
    "\n",
    "#### Steps:\n",
    "<ul>\n",
    "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
    "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
    "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
    "</ul>\n",
    "\n",
    "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "_cell_guid": "f0acfc44-eb2a-4356-ad03-d0c12807acd7",
    "_kg_hide-input": true,
    "_uuid": "e3a2b89752681164f14c8273452fc66734d7f41b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42936</th>\n",
       "      <td>0.431895</td>\n",
       "      <td>-0.198913</td>\n",
       "      <td>-12.980943</td>\n",
       "      <td>6.720508</td>\n",
       "      <td>-13.455636</td>\n",
       "      <td>8.698610</td>\n",
       "      <td>-11.479552</td>\n",
       "      <td>-2.681519</td>\n",
       "      <td>-14.019291</td>\n",
       "      <td>8.218191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.843303</td>\n",
       "      <td>2.549628</td>\n",
       "      <td>-0.532228</td>\n",
       "      <td>-0.235096</td>\n",
       "      <td>0.673209</td>\n",
       "      <td>0.226598</td>\n",
       "      <td>-0.006168</td>\n",
       "      <td>-1.185696</td>\n",
       "      <td>-0.747361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6581</th>\n",
       "      <td>2.474547</td>\n",
       "      <td>-1.569364</td>\n",
       "      <td>0.649933</td>\n",
       "      <td>-0.660716</td>\n",
       "      <td>1.283827</td>\n",
       "      <td>2.169457</td>\n",
       "      <td>-0.938512</td>\n",
       "      <td>0.546842</td>\n",
       "      <td>-0.285284</td>\n",
       "      <td>0.083293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217546</td>\n",
       "      <td>-0.407082</td>\n",
       "      <td>-0.894580</td>\n",
       "      <td>-0.060158</td>\n",
       "      <td>0.365568</td>\n",
       "      <td>0.306398</td>\n",
       "      <td>-0.593129</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>0.585991</td>\n",
       "      <td>-1.763502</td>\n",
       "      <td>-1.864708</td>\n",
       "      <td>-0.724766</td>\n",
       "      <td>2.435963</td>\n",
       "      <td>0.734872</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>-0.298395</td>\n",
       "      <td>-0.111806</td>\n",
       "      <td>0.115256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445434</td>\n",
       "      <td>-0.162327</td>\n",
       "      <td>-0.033367</td>\n",
       "      <td>-0.526250</td>\n",
       "      <td>0.082913</td>\n",
       "      <td>0.495219</td>\n",
       "      <td>-0.362530</td>\n",
       "      <td>-0.019357</td>\n",
       "      <td>-0.241960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66955</th>\n",
       "      <td>-0.190119</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>1.318256</td>\n",
       "      <td>-0.931846</td>\n",
       "      <td>1.180751</td>\n",
       "      <td>-0.333078</td>\n",
       "      <td>-1.871208</td>\n",
       "      <td>-0.428447</td>\n",
       "      <td>-1.225269</td>\n",
       "      <td>0.141701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.594350</td>\n",
       "      <td>-0.500435</td>\n",
       "      <td>-0.878553</td>\n",
       "      <td>0.164914</td>\n",
       "      <td>0.361788</td>\n",
       "      <td>-0.059956</td>\n",
       "      <td>0.966398</td>\n",
       "      <td>-0.018525</td>\n",
       "      <td>0.018058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49502</th>\n",
       "      <td>-0.264540</td>\n",
       "      <td>-0.084308</td>\n",
       "      <td>-1.475255</td>\n",
       "      <td>1.500748</td>\n",
       "      <td>0.531491</td>\n",
       "      <td>-0.722565</td>\n",
       "      <td>-0.378363</td>\n",
       "      <td>-0.348136</td>\n",
       "      <td>-0.061568</td>\n",
       "      <td>0.988852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117683</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.395754</td>\n",
       "      <td>0.025456</td>\n",
       "      <td>-0.268201</td>\n",
       "      <td>-0.150557</td>\n",
       "      <td>0.355148</td>\n",
       "      <td>0.173096</td>\n",
       "      <td>0.106531</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       scaled_amount  scaled_time         V1        V2         V3        V4  \\\n",
       "42936       0.431895    -0.198913 -12.980943  6.720508 -13.455636  8.698610   \n",
       "6581        2.474547    -1.569364   0.649933 -0.660716   1.283827  2.169457   \n",
       "3789        0.585991    -1.763502  -1.864708 -0.724766   2.435963  0.734872   \n",
       "66955      -0.190119     0.253623   1.318256 -0.931846   1.180751 -0.333078   \n",
       "49502      -0.264540    -0.084308  -1.475255  1.500748   0.531491 -0.722565   \n",
       "\n",
       "              V5        V6         V7        V8  ...       V20       V21  \\\n",
       "42936 -11.479552 -2.681519 -14.019291  8.218191  ... -0.843303  2.549628   \n",
       "6581   -0.938512  0.546842  -0.285284  0.083293  ...  0.217546 -0.407082   \n",
       "3789    0.934271 -0.298395  -0.111806  0.115256  ... -0.445434 -0.162327   \n",
       "66955  -1.871208 -0.428447  -1.225269  0.141701  ... -0.594350 -0.500435   \n",
       "49502  -0.378363 -0.348136  -0.061568  0.988852  ... -0.117683 -0.139097   \n",
       "\n",
       "            V22       V23       V24       V25       V26       V27       V28  \\\n",
       "42936 -0.532228 -0.235096  0.673209  0.226598 -0.006168 -1.185696 -0.747361   \n",
       "6581  -0.894580 -0.060158  0.365568  0.306398 -0.593129  0.029920  0.064128   \n",
       "3789  -0.033367 -0.526250  0.082913  0.495219 -0.362530 -0.019357 -0.241960   \n",
       "66955 -0.878553  0.164914  0.361788 -0.059956  0.966398 -0.018525  0.018058   \n",
       "49502 -0.395754  0.025456 -0.268201 -0.150557  0.355148  0.173096  0.106531   \n",
       "\n",
       "       Class  \n",
       "42936      1  \n",
       "6581       0  \n",
       "3789       0  \n",
       "66955      0  \n",
       "49502      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
    "\n",
    "# Lets shuffle the data before creating the subsamples\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77198464-c0f8-4694-ac0b-4b29b94d0da3",
    "_uuid": "b6818122806657e7accb8be1f4bf17086bb9b149"
   },
   "source": [
    "##  Equally Distributing and Correlating: \n",
    "<a id=\"correlating\"></a>\n",
    "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "_cell_guid": "73454100-dc69-49fd-b1b2-f72e326bca5d",
    "_kg_hide-input": true,
    "_uuid": "68b42e92df59f10fbd3ba700389796c4506af604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset\n",
      "Class\n",
      "0    0.675824\n",
      "1    0.324176\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "\n",
    "sns.countplot(x='Class', data=new_df, palette=colors)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "# --- ADD THESE LINES HERE ---\n",
    "plt.savefig(\"undersampled_class_distribution.png\") # Choose a descriptive filename\n",
    "plt.close() # Close the figure to free up memory\n",
    "# --- END ADDITION ---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0abc31ee-a78e-43af-822f-f06772d00c1c",
    "_uuid": "88477bac6687f110e9d64ec22837c250d85d2a2b"
   },
   "source": [
    "<h3> Correlation Matrices </h3>\n",
    "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n",
    "\n",
    "### Summary and Explanation: \n",
    "<ul>\n",
    "<li><b>Negative Correlations: </b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  </li>\n",
    "<li> <b> Positive Correlations: </b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. </li>\n",
    "<li> <b>BoxPlots: </b>  We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "_cell_guid": "9f353623-9435-4bb2-b854-b4a201ec7dd9",
    "_kg_hide-input": true,
    "_uuid": "e2f417c5d7c633a1e3cdfaa78acd6bd77a38400e"
   },
   "outputs": [],
   "source": [
    "# Make sure we use the subsample in our correlation\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "\n",
    "# --- ADD THESE LINES HERE ---\n",
    "plt.savefig(\"correlation_matrix_undersampled.png\") # This filename was recommended earlier\n",
    "plt.close('all') # Use 'all' to ensure both subplots (and the entire figure) are closed\n",
    "# --- END ADDITION ---\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "_cell_guid": "2f02c21f-daa3-4251-a8e9-acad09a5ce0f",
    "_kg_hide-input": true,
    "_uuid": "318d0e7e0443f99139be21c00a7abc663be26385"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title('V17 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
    "axes[1].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
    "axes[3].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "\n",
    "plt.savefig(\"negative_corr_boxplots.png\") \n",
    "plt.close('all') \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "_cell_guid": "b457b10e-c17c-4cb2-9719-6d4128377c9f",
    "_kg_hide-input": true,
    "_uuid": "7bfc46c028f8602ee949de83629082633aa47b2c"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\n",
    "axes[1].set_title('V4 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title('V2 vs Class Positive Correlation')\n",
    "\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\n",
    "axes[3].set_title('V19 vs Class Positive Correlation')\n",
    "\n",
    "plt.savefig(\"positive_corr_boxplots.png\") # This filename was recommended earlier\n",
    "plt.close('all') # Ensure the entire figure with all subplots is closed\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93e56c89-185e-40d2-9ccc-29b123feb5a6",
    "_uuid": "a721282c0f44ec8030bbad6d0220091bde8cbec8"
   },
   "source": [
    "## Anomaly Detection:\n",
    "<a id=\"anomaly\"></a>\n",
    "<img src=\"https://discourse-cdn-sjc1.com/business6/uploads/analyticsvidhya/original/2X/d/d11281b44c2e440b36aaf29156b5032105d2d06b.png\">\n",
    "\n",
    "\n",
    "Our main aim in this section is to remove \"extreme outliers\" from features that have a high correlation with our classes. This will have a positive impact on the accuracy of our models.  <br><br>\n",
    "\n",
    "\n",
    "### Interquartile Range Method:\n",
    "<ul>\n",
    "<li> <b>Interquartile Range (IQR): </b> We calculate this by the difference between the 75th percentile and 25th percentile. Our aim is to create a threshold beyond the 75th and 25th percentile that in case some instance pass this threshold the instance will be deleted.  </li>\n",
    "<li> <b>Boxplots: </b> Besides easily seeing the 25th and 75th percentiles (both end of the squares) it is also easy to see extreme outliers (points beyond the lower and higher extreme). </li>\n",
    "</ul>\n",
    "\n",
    "### Outlier Removal Tradeoff:\n",
    "We have to be careful as to how far do we want the threshold for removing outliers. We determine the threshold by multiplying a number (ex: 1.5) by the (Interquartile Range). The higher this threshold is, the less outliers will detect (multiplying by a higher number ex: 3), and the lower this threshold is the more outliers it will detect.  <br><br>\n",
    "\n",
    "**The Tradeoff: **\n",
    "The lower the threshold the more outliers it will remove however, we want to focus more on \"extreme outliers\" rather than just outliers. Why? because we might run the risk of information loss which will cause our models to have a lower accuracy. You can play with this threshold and see how it affects the accuracy of our classification models.\n",
    "\n",
    "\n",
    "### Summary:\n",
    "<ul>\n",
    "<li> <b> Visualize Distributions: </b> We first start by visualizing the distribution of the feature we are going to use to eliminate some of the outliers. V14 is the only feature that has a Gaussian distribution compared to features V12 and V10. </li>\n",
    "<li><b>Determining the threshold: </b> After we decide which number we will use to multiply with the iqr (the lower more outliers removed), we will proceed in determining the upper and lower thresholds by substrating q25 - threshold (lower extreme threshold) and adding q75 + threshold (upper extreme threshold). </li>\n",
    "<li> <b>Conditional Dropping: </b> Lastly, we create a conditional dropping stating that if the \"threshold\" is exceeded in both extremes, the instances will be removed. </li>\n",
    "<li> <b> Boxplot Representation: </b> Visualize through the boxplot that the number of \"extreme outliers\" have been reduced to a considerable amount. </li>\n",
    "</ul>\n",
    "\n",
    "**Note:** After implementing outlier reduction our accuracy has been improved by over 3%! Some outliers can distort the accuracy of our models but remember, we have to avoid an extreme amount of information loss or else our model runs the risk of underfitting.\n",
    "\n",
    "\n",
    "**Reference**: More information on Interquartile Range Method: <a src=\"https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/\"> How to Use Statistics to Identify Outliers in Data </a> by Jason Brownless (Machine Learning Mastery blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "_cell_guid": "9c690dfa-8fed-44e5-99f5-ff4eb6f87f16",
    "_kg_hide-input": true,
    "_uuid": "b6963900379db5b0d4adf92f8c7f959164e9119f"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "\n",
    "v14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\n",
    "ax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "v12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\n",
    "ax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "\n",
    "v10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\n",
    "ax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n",
    "\n",
    "plt.savefig(\"fraud_feature_dist_initial.png\") \n",
    "plt.close('all') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "_cell_guid": "2e19fe33-f85a-4ffd-8e4a-807d0e0fb992",
    "_kg_hide-input": true,
    "_uuid": "21e43406e62a9561fba2f065ce15a8d87a1bf389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances before outlier removal in this block: 728\n",
      "Quartile 25: -9.840594332582144 | Quartile 75: -4.72720013534106\n",
      "iqr: 5.113394197241084\n",
      "Cut Off: 7.670091295861626\n",
      "V14 Lower: -17.51068562844377\n",
      "V14 Upper: 2.942891160520566\n",
      "Feature V14 Outliers for Fraud Cases: 6\n",
      "V10 outliers:[-18.8220867423816, -18.4937733551053, -17.6206343516773, -17.7216383537133, -18.0499976898594, -19.2143254902614]\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "V12 Lower: -17.11606709151623\n",
      "V12 Upper: 4.714589086897137\n",
      "V12 outliers: []\n",
      "Feature V12 Outliers for Fraud Cases: 0\n",
      "Number of Instances after outliers removal: 719\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "V10 Lower: -16.471059260947914\n",
      "V10 Upper: 5.250546408231316\n",
      "V10 outliers: [-16.6496281595399, -18.2711681738888]\n",
      "Feature V10 Outliers for Fraud Cases: 2\n",
      "Number of Instances after outliers removal: 711\n",
      "\n",
      "Total number of outliers removed in this block: 17\n"
     ]
    }
   ],
   "source": [
    "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
    "\n",
    "# Capture the number of instances BEFORE any outlier removal in this block\n",
    "len_before_outlier_removal = len(new_df)\n",
    "print(f\"Instances before outlier removal in this block: {len_before_outlier_removal}\")\n",
    "\n",
    "\n",
    "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n",
    "v14_iqr = q75 - q25\n",
    "print('iqr: {}'.format(v14_iqr))\n",
    "\n",
    "v14_cut_off = v14_iqr * 1.5\n",
    "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
    "print('Cut Off: {}'.format(v14_cut_off))\n",
    "print('V14 Lower: {}'.format(v14_lower))\n",
    "print('V14 Upper: {}'.format(v14_upper))\n",
    "\n",
    "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
    "print('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "print('V10 outliers:{}'.format(outliers))\n",
    "\n",
    "new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
    "print('----' * 44)\n",
    "\n",
    "# -----> V12 removing outliers from fraud transactions\n",
    "v12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
    "v12_iqr = q75 - q25\n",
    "\n",
    "v12_cut_off = v12_iqr * 1.5\n",
    "v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\n",
    "print('V12 Lower: {}'.format(v12_lower))\n",
    "print('V12 Upper: {}'.format(v12_upper))\n",
    "outliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
    "print('V12 outliers: {}'.format(outliers))\n",
    "print('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "new_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\n",
    "print('Number of Instances after outliers removal: {}'.format(len(new_df)))\n",
    "print('----' * 44)\n",
    "\n",
    "\n",
    "# Removing outliers V10 Feature\n",
    "v10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
    "v10_iqr = q75 - q25\n",
    "\n",
    "v10_cut_off = v10_iqr * 1.5\n",
    "v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\n",
    "print('V10 Lower: {}'.format(v10_lower))\n",
    "print('V10 Upper: {}'.format(v10_upper))\n",
    "outliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
    "print('V10 outliers: {}'.format(outliers))\n",
    "print('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\n",
    "new_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n",
    "print('Number of Instances after outliers removal: {}'.format(len(new_df)))     \n",
    "\n",
    "# This calculates the total number of outliers removed across all V14, V12, V10 steps\n",
    "num_outliers_removed = len_before_outlier_removal - len(new_df)\n",
    "print(f\"\\nTotal number of outliers removed in this block: {num_outliers_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "_cell_guid": "66e44398-7c91-4cce-9778-4512cb838973",
    "_kg_hide-input": true,
    "_uuid": "ac80d9cfb07f1865094a8d460ae801750e93d694"
   },
   "outputs": [],
   "source": [
    "f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n",
    "\n",
    "colors = ['#B3F9C5', '#f9c5b3']\n",
    "# Boxplots with outliers removed\n",
    "# Feature V14\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\n",
    "ax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "# Feature 12\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\n",
    "ax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "# Feature V10\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\n",
    "ax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\n",
    "ax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n",
    "            arrowprops=dict(facecolor='black'),\n",
    "            fontsize=14)\n",
    "\n",
    "plt.savefig(\"fraud_feature_dist_post_outlier.png\")\n",
    "plt.close('all') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "74903f3b-dc6b-40ba-abc8-86c3df5ca46e",
    "_uuid": "0b365b10bd363c23068accc448509ced879f1670"
   },
   "source": [
    "<h2>Dimensionality Reduction and Clustering: </h2>\n",
    "<a id=\"clustering\"></a>\n",
    "\n",
    "<h3>Understanding t-SNE:  </h3>\n",
    "In order to understand this algorithm you have to understand the following terms: <br>\n",
    "<ul>\n",
    "<li> <b> Euclidean Distance </b></li>\n",
    "<li> <b>Conditional Probability</b> </li>\n",
    "<li><b>Normal and T-Distribution Plots</b> </li>\n",
    "</ul> \n",
    "\n",
    "**Note:** If you want a simple instructive video look at <a href=\"https://www.youtube.com/watch?v=NEaUSP4YerM\"> StatQuest: t-SNE, Clearly Explained </a> by Joshua Starmer\n",
    "\n",
    "\n",
    "<h3> Summary: </h3>\n",
    "<ul> \n",
    "<li>t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset. </li>\n",
    "<li> Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)</li>\n",
    "<li> This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "_cell_guid": "f83cde6b-90d0-4e9d-ac63-fb69780431b2",
    "_kg_hide-input": true,
    "_uuid": "af3027e7df67b75c92c88d597003632e285c9bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE took 2.9 s\n",
      "PCA took 0.0045 s\n",
      "Truncated SVD took 0.0037 s\n"
     ]
    }
   ],
   "source": [
    "    # New_df is from the random undersample data (fewer instances)\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "\n",
    "# T-SNE Implementation\n",
    "t0 = time.time()\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"T-SNE took {:.2} s\".format(t1 - t0))\n",
    "\n",
    "# PCA Implementation\n",
    "t0 = time.time()\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"PCA took {:.2} s\".format(t1 - t0))\n",
    "\n",
    "# TruncatedSVD\n",
    "t0 = time.time()\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
    "t1 = time.time()\n",
    "print(\"Truncated SVD took {:.2} s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "_cell_guid": "07015ae5-f7ac-4d64-8f41-1e4b7c9dd2ac",
    "_kg_hide-input": true,
    "_uuid": "084f2a7421c2212082491d2a90e65d65c52b434a"
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
    "# labels = ['No Fraud', 'Fraud']\n",
    "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
    "\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax1.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax2.set_title('PCA', fontsize=14)\n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# TruncatedSVD scatter plot\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax3.set_title('Truncated SVD', fontsize=14)\n",
    "\n",
    "ax3.grid(True)\n",
    "\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.savefig(\"dimensionality_reduction_clusters.png\") \n",
    "plt.close('all') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb2c480a-090a-4cfb-b12e-3b74c325826c",
    "_uuid": "1b63bfd92008043cc1a336f924c835e73792f6d8"
   },
   "source": [
    "<h2> Classifiers (UnderSampling):  </h2>\n",
    "<a id=\"classifiers\"></a>\n",
    "In this section we will train four types of classifiers and decide which classifier will be more effective in detecting <b>fraud transactions</b>.  Before we have to split our data into training and testing sets and separate the features from the labels.\n",
    "\n",
    "## Summary: \n",
    "<ul>\n",
    "<li> <b> Logistic Regression </b> classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression) </li>\n",
    "<li><b> GridSearchCV </b> is used to determine the paremeters that gives the best predictive score for the classifiers. </li>\n",
    "<li> Logistic Regression has the best Receiving Operating Characteristic score  (ROC), meaning that LogisticRegression pretty accurately separates <b> fraud </b> and <b> non-fraud </b> transactions.</li>\n",
    "</ul>\n",
    "\n",
    "## Learning Curves:\n",
    "<ul>\n",
    "<li>The <b>wider the  gap</b>  between the training score and the cross validation score, the more likely your model is <b>overfitting (high variance)</b>.</li>\n",
    "<li> If the score is low in both training and cross-validation sets</b> this is an indication that our model is <b>underfitting (high bias)</b></li>\n",
    "<li><b> Logistic Regression Classifier</b>  shows the best score in both training and cross-validating sets.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "_cell_guid": "85ce8738-7599-4b06-a722-5c0ed073599b",
    "_kg_hide-input": true,
    "_uuid": "e3751d88766a982119e522e27a9c0c647f20af85"
   },
   "outputs": [],
   "source": [
    "# Undersampling before cross validating (prone to overfit)\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "_cell_guid": "288a65b7-8b86-44b1-973d-38dbcfe82bbb",
    "_kg_hide-input": true,
    "_uuid": "fb0a479efaa7147d6702c2c24083f1118621863f"
   },
   "outputs": [],
   "source": [
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "_cell_guid": "bccd5685-a979-451e-85b3-1cb968523540",
    "_kg_hide-input": true,
    "_uuid": "28f5178089d2d133b9e7478c1c7dc7a1f98aabee"
   },
   "outputs": [],
   "source": [
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Data Prep & EDA Summary\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "with mlflow.start_run(run_name=\"Data_Prep_and_EDA_Summary\"):\n",
    "    # --- Log Parameters (Key details of your data prep and EDA process) ---\n",
    "    mlflow.log_param(\"data_source\", \"transactions.csv\")\n",
    "    mlflow.log_param(\"feature_scaling_method\", \"RobustScaler\")\n",
    "    mlflow.log_param(\"train_test_split_random_state\", 42) # Assuming you used random_state=42 for initial split\n",
    "    mlflow.log_param(\"initial_split_ratio\", \"80/20\")\n",
    "    mlflow.log_param(\"undersampling_method\", \"RandomUnderSampler\")\n",
    "    mlflow.log_param(\"outlier_removal_method\", \"IQR (1.5 * IQR)\") # Specify the method if you used one\n",
    "    mlflow.log_param(\"dimensionality_reduction_methods\", \"TSNE, PCA, TruncatedSVD\") # If these were applied\n",
    "    mlflow.log_param(\"reduction_random_state\", 42) # If using 42 for TSNE/PCA/SVD algorithms\n",
    "\n",
    "    # --- Log Metrics (Numerical summaries) ---\n",
    "    # These metrics assume you have defined these variables in earlier cells\n",
    "    if 'original_total_transactions' in locals(): # Check if variable exists before logging\n",
    "        mlflow.log_metric(\"original_total_transactions\", original_total_transactions)\n",
    "        mlflow.log_param(\"original_fraud_count\", original_fraud_count)\n",
    "        mlflow.log_param(\"original_non_fraud_count\", original_non_fraud_count)\n",
    "        mlflow.log_metric(\"original_fraud_percentage\", round(original_fraud_count/original_total_transactions * 100, 2))\n",
    "    if 'undersampled_fraud_count' in locals(): # Check if variable exists before logging\n",
    "        mlflow.log_param(\"undersampled_fraud_count\", undersampled_fraud_count)\n",
    "        mlflow.log_param(\"undersampled_non_fraud_count\", undersampled_non_fraud_count)\n",
    "        mlflow.log_metric(\"undersampled_total_rows\", undersampled_fraud_count + undersampled_non_fraud_count)\n",
    "\n",
    "\n",
    "\n",
    "    # --- Log Artifacts (Your saved plots) ---\n",
    "    mlflow.log_artifact(\"original_class_distribution.png\")\n",
    "    mlflow.log_artifact(\"undersampled_class_distribution.png\")\n",
    "    mlflow.log_artifact(\"correlation_matrix_undersampled.png\")\n",
    "    mlflow.log_artifact(\"negative_corr_boxplots.png\")\n",
    "    mlflow.log_artifact(\"positive_corr_boxplots.png\")\n",
    "    mlflow.log_artifact(\"fraud_feature_dist_initial.png\")\n",
    "    mlflow.log_artifact(\"fraud_feature_dist_post_outlier.png\")\n",
    "    mlflow.log_artifact(\"dimensionality_reduction_clusters.png\")\n",
    "\n",
    "    \n",
    "    os.remove(\"original_class_distribution.png\")\n",
    "    os.remove(\"undersampled_class_distribution.png\")\n",
    "    os.remove(\"correlation_matrix_undersampled.png\")\n",
    "    os.remove(\"negative_corr_boxplots.png\")\n",
    "    os.remove(\"positive_corr_boxplots.png\")\n",
    "    os.remove(\"fraud_feature_dist_initial.png\")\n",
    "    os.remove(\"fraud_feature_dist_post_outlier.png\")\n",
    "    os.remove(\"dimensionality_reduction_clusters.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "_cell_guid": "7810d0b9-b4e5-4b7f-909b-c127365b167c",
    "_kg_hide-input": true,
    "_uuid": "8dd4ea07fd60973fccabc2d46af28a09b0de9178"
   },
   "outputs": [],
   "source": [
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "_cell_guid": "eb37c0f6-9cfe-48b6-92d3-475d5e6767a6",
    "_kg_hide-input": true,
    "_uuid": "fe129af379caccc5428cf1836e6c96bd32e68feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 95.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 95.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 95.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 91.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)  \n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "_cell_guid": "a1c35773-f4c7-4caf-9911-532784c9eae0",
    "_kg_hide-input": true,
    "_uuid": "d15b1ab16737358806e34c48dc57aa238cf0cfd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "2025/06/19 18:46:23 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Best Params: {'C': 0.001, 'penalty': 'l2'}\n",
      "Logistic Regression CV Accuracy: 95.95%\n",
      "--- Logistic Regression training complete ---\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Training K-Nearest Neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/19 18:46:44 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Best Params: {'algorithm': 'auto', 'n_neighbors': 4}\n",
      "K-Nearest Neighbors CV Accuracy: 95.25%\n",
      "--- K-Nearest Neighbors training complete ---\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Training Support Vector Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/Vimal Karthik/mcpservers/mcpcrit/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "2025/06/19 18:47:02 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Best Params: {'C': 0.7, 'kernel': 'rbf'}\n",
      "SVC CV Accuracy: 95.25%\n",
      "--- SVC training complete ---\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Training Decision Tree Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/19 18:47:20 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Best Params: {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5}\n",
      "Decision Tree CV Accuracy: 95.25%\n",
      "--- Decision Tree training complete ---\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "import numpy as np # For cross_val_score.mean()/std()\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# ==============================================================================\n",
    "# Logistic Regression Training\n",
    "# ==============================================================================\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic_Regression_GridSearch\"):\n",
    "    mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "    mlflow.log_params({\"grid_penalty_options\": str(log_reg_params['penalty']),\n",
    "                        \"grid_C_options\": str(log_reg_params['C'])})\n",
    "\n",
    "    grid_log_reg = GridSearchCV(LogisticRegression(solver='liblinear', random_state=42, max_iter=10000),\n",
    "                                log_reg_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"Training Logistic Regression...\")\n",
    "    grid_log_reg.fit(X_train, y_train)\n",
    "    log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "    mlflow.log_params(grid_log_reg.best_params_)\n",
    "    log_reg_cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    mlflow.log_metric(\"cv_accuracy_mean\", np.mean(log_reg_cv_scores))\n",
    "    mlflow.log_metric(\"cv_accuracy_std\", np.std(log_reg_cv_scores))\n",
    "    \n",
    "    # --- CHANGE HERE for Logistic Regression MLflow logging ---\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=log_reg,\n",
    "        name=\"logistic_regression_model\", # Changed 'artifact_path' to 'name'\n",
    "        input_example=X_train[:5]        # Added 'input_example' using the current X_train\n",
    "    )\n",
    "\n",
    "    print(f\"Logistic Regression Best Params: {grid_log_reg.best_params_}\")\n",
    "    print(f\"Logistic Regression CV Accuracy: {round(np.mean(log_reg_cv_scores) * 100, 2)}%\")\n",
    "    print(\"--- Logistic Regression training complete ---\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# K-Nearest Neighbors Training\n",
    "# ==============================================================================\n",
    "knears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "with mlflow.start_run(run_name=\"KNearest_Neighbors_GridSearch\"):\n",
    "    mlflow.log_param(\"model_type\", \"K-Nearest Neighbors\")\n",
    "    mlflow.log_params({\"grid_n_neighbors_options\": str(knears_params['n_neighbors']),\n",
    "                        \"grid_algorithm_options\": str(knears_params['algorithm'])})\n",
    "\n",
    "    grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"Training K-Nearest Neighbors...\")\n",
    "    grid_knears.fit(X_train, y_train)\n",
    "    knears_neighbors = grid_knears.best_estimator_\n",
    "\n",
    "    mlflow.log_params(grid_knears.best_params_)\n",
    "    knears_cv_scores = cross_val_score(knears_neighbors, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    mlflow.log_metric(\"cv_accuracy_mean\", np.mean(knears_cv_scores))\n",
    "    mlflow.log_metric(\"cv_accuracy_std\", np.std(knears_cv_scores))\n",
    "    \n",
    "    # --- CHANGE HERE for K-Nearest Neighbors MLflow logging ---\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=knears_neighbors,\n",
    "        name=\"k_nearest_neighbors_model\", # Changed 'artifact_path' to 'name'\n",
    "        input_example=X_train[:5]        # Added 'input_example' using the current X_train\n",
    "    )\n",
    "\n",
    "    print(f\"K-Nearest Neighbors Best Params: {grid_knears.best_params_}\")\n",
    "    print(f\"K-Nearest Neighbors CV Accuracy: {round(np.mean(knears_cv_scores) * 100, 2)}%\")\n",
    "    print(\"--- K-Nearest Neighbors training complete ---\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Support Vector Classifier Training\n",
    "# ==============================================================================\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
    "\n",
    "with mlflow.start_run(run_name=\"SVC_GridSearch\"):\n",
    "    mlflow.log_param(\"model_type\", \"Support Vector Classifier\")\n",
    "    mlflow.log_params({\"grid_C_options\": str(svc_params['C']),\n",
    "                        \"grid_kernel_options\": str(svc_params['kernel'])})\n",
    "\n",
    "    grid_svc = GridSearchCV(SVC(probability=True, random_state=42, max_iter=10000),\n",
    "                            svc_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"Training Support Vector Classifier...\")\n",
    "    grid_svc.fit(X_train, y_train)\n",
    "    svc = grid_svc.best_estimator_\n",
    "\n",
    "    mlflow.log_params(grid_svc.best_params_)\n",
    "    svc_cv_scores = cross_val_score(svc, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    mlflow.log_metric(\"cv_accuracy_mean\", np.mean(svc_cv_scores))\n",
    "    mlflow.log_metric(\"cv_accuracy_std\", np.std(svc_cv_scores))\n",
    "    \n",
    "    # --- CHANGE HERE for Support Vector Classifier MLflow logging ---\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=svc,\n",
    "        name=\"svc_model\", # Changed 'artifact_path' to 'name'\n",
    "        input_example=X_train[:5]        # Added 'input_example' using the current X_train\n",
    "    )\n",
    "\n",
    "    print(f\"SVC Best Params: {grid_svc.best_params_}\")\n",
    "    print(f\"SVC CV Accuracy: {round(np.mean(svc_cv_scores) * 100, 2)}%\")\n",
    "    print(\"--- SVC training complete ---\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DecisionTree Classifier Training\n",
    "# ==============================================================================\n",
    "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\n",
    "               \"min_samples_leaf\": list(range(5,7,1))}\n",
    "\n",
    "with mlflow.start_run(run_name=\"DecisionTree_GridSearch\"):\n",
    "    mlflow.log_param(\"model_type\", \"Decision Tree Classifier\")\n",
    "    mlflow.log_params({\"grid_criterion_options\": str(tree_params['criterion']),\n",
    "                        \"grid_max_depth_options\": str(tree_params['max_depth']),\n",
    "                        \"grid_min_samples_leaf_options\": str(tree_params['min_samples_leaf'])})\n",
    "\n",
    "    grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                             tree_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(\"Training Decision Tree Classifier...\")\n",
    "    grid_tree.fit(X_train, y_train)\n",
    "    tree_clf = grid_tree.best_estimator_\n",
    "\n",
    "    mlflow.log_params(grid_tree.best_params_)\n",
    "    tree_cv_scores = cross_val_score(tree_clf, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    mlflow.log_metric(\"cv_accuracy_mean\", np.mean(tree_cv_scores))\n",
    "    mlflow.log_metric(\"cv_accuracy_std\", np.std(tree_cv_scores))\n",
    "    \n",
    "    # --- CHANGE HERE for Decision Tree Classifier MLflow logging ---\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=tree_clf,\n",
    "        name=\"decision_tree_model\", # Changed 'artifact_path' to 'name'\n",
    "        input_example=X_train[:5]        # Added 'input_example' using the current X_train\n",
    "    )\n",
    "\n",
    "    print(f\"Decision Tree Best Params: {grid_tree.best_params_}\")\n",
    "    print(f\"Decision Tree CV Accuracy: {round(np.mean(tree_cv_scores) * 100, 2)}%\")\n",
    "    print(\"--- Decision Tree training complete ---\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "_cell_guid": "7f327bcd-335f-4e49-af07-fc4214dbcbdc",
    "_kg_hide-input": true,
    "_uuid": "1b2108bf377b924ed8a6efe580d9e162a132cd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross Validation Score:  95.95%\n",
      "Knears Neighbors Cross Validation Score 95.25%\n",
      "Support Vector Classifier Cross Validation Score 95.25%\n",
      "DecisionTree Classifier Cross Validation Score 95.25%\n"
     ]
    }
   ],
   "source": [
    "# Overfitting Case\n",
    "\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "\n",
    "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
    "\n",
    "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
    "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "_cell_guid": "38e430ef-0160-47a1-9b6f-11ff62c5ecc0",
    "_kg_hide-input": true,
    "_uuid": "eeb5736b279bb8fa3804689a175394f216ec4f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline cross-validation for all classifiers...\n",
      "\n",
      "--- Evaluating pipeline for Logistic Regression ---\n",
      "Results for Logistic Regression:\n",
      "  Accuracy (Mean): 0.9507 (+/- 0.0213)\n",
      "  Precision (Mean): 0.9217 (+/- 0.0527)\n",
      "  Recall (Mean): 0.9290 (+/- 0.0443)\n",
      "  F1 (Mean): 0.9239 (+/- 0.0330)\n",
      "  Roc Auc (Mean): 0.9595 (+/- 0.0383)\n",
      "\n",
      "--- Evaluating pipeline for K-Nearest Neighbors ---\n",
      "Results for K-Nearest Neighbors:\n",
      "  Accuracy (Mean): 0.9507 (+/- 0.0089)\n",
      "  Precision (Mean): 0.9711 (+/- 0.0259)\n",
      "  Recall (Mean): 0.8746 (+/- 0.0433)\n",
      "  F1 (Mean): 0.9191 (+/- 0.0166)\n",
      "  Roc Auc (Mean): 0.9505 (+/- 0.0199)\n",
      "\n",
      "--- Evaluating pipeline for Support Vector Classifier ---\n",
      "Results for Support Vector Classifier:\n",
      "  Accuracy (Mean): 0.9507 (+/- 0.0142)\n",
      "  Precision (Mean): 0.9829 (+/- 0.0229)\n",
      "  Recall (Mean): 0.8638 (+/- 0.0560)\n",
      "  F1 (Mean): 0.9179 (+/- 0.0261)\n",
      "  Roc Auc (Mean): 0.9763 (+/- 0.0100)\n",
      "\n",
      "--- Evaluating pipeline for Decision Tree Classifier ---\n",
      "Results for Decision Tree Classifier:\n",
      "  Accuracy (Mean): 0.9490 (+/- 0.0150)\n",
      "  Precision (Mean): 0.9536 (+/- 0.0120)\n",
      "  Recall (Mean): 0.8856 (+/- 0.0572)\n",
      "  F1 (Mean): 0.9170 (+/- 0.0276)\n",
      "  Roc Auc (Mean): 0.9558 (+/- 0.0118)\n",
      "\n",
      "All pipeline cross-validation evaluations complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 58 - Imbalanced Cross-Validation Pipeline Evaluation with MLflow Logging\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Alias to avoid conflict with sklearn.pipeline.Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# IMPORTANT CHANGE: Added cross_validate to imports\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from collections import Counter # To show class distribution if needed for debugging\n",
    "\n",
    "with mlflow.start_run(run_name=\"Pipeline_Cross_Validation_Evaluation\"):\n",
    "    mlflow.log_param(\"pipeline_type\", \"Imbalanced-Learn Pipeline with RandomUnderSampler\")\n",
    "    mlflow.log_param(\"cross_validation_strategy\", \"StratifiedKFold\")\n",
    "    mlflow.log_param(\"cross_validation_folds\", 5) # Assuming cv=5\n",
    "    mlflow.log_param(\"random_undersampler_state\", 42) # For RandomUnderSampler\n",
    "\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": log_reg,\n",
    "        \"K-Nearest Neighbors\": knears_neighbors,\n",
    "        \"Support Vector Classifier\": svc,\n",
    "        \"Decision Tree Classifier\": tree_clf\n",
    "    }\n",
    "\n",
    "    # Define scorers for cross_validate to get all desired metrics\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'f1': make_scorer(f1_score),\n",
    "        'roc_auc': make_scorer(roc_auc_score, needs_threshold=True) # needs_threshold for ROC AUC\n",
    "    }\n",
    "\n",
    "    print(\"Starting pipeline cross-validation for all classifiers...\")\n",
    "\n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"\\n--- Evaluating pipeline for {name} ---\")\n",
    "        # Create the pipeline: Undersampler then Classifier\n",
    "        pipeline = ImbPipeline([\n",
    "            ('undersample', RandomUnderSampler(random_state=42)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "\n",
    "        # Perform cross-validation with StratifiedKFold on the *original* X_train, y_train\n",
    "        # This ensures undersampling is done correctly per fold.\n",
    "        cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        # Added return_train_score=False as we usually only care about test scores in CV\n",
    "        cv_results = cross_validate(pipeline, X_train, y_train, cv=cv_strategy,\n",
    "                                    scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "\n",
    "        print(f\"Results for {name}:\")\n",
    "       \n",
    "        for metric_name in scoring.keys():\n",
    "            scores_array = cv_results[f'test_{metric_name}'] \n",
    "            mean_score = np.mean(scores_array)\n",
    "            std_score = np.std(scores_array)\n",
    "            mlflow.log_metric(f\"cv_pipeline_{name.replace(' ', '_').lower()}_{metric_name}_mean\", mean_score)\n",
    "            mlflow.log_metric(f\"cv_pipeline_{name.replace(' ', '_').lower()}_{metric_name}_std\", std_score)\n",
    "            print(f\"  {metric_name.replace('_', ' ').title()} (Mean): {mean_score:.4f} (+/- {std_score:.4f})\")\n",
    "\n",
    "    print(\"\\nAll pipeline cross-validation evaluations complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "_cell_guid": "bb72803c-3ea3-40cd-8ac3-399540ab7f5a",
    "_kg_hide-input": true,
    "_uuid": "a12fb2f7e104931bb78e1bd6cfc5a516c970708b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLflow run for final test set evaluation and plots...\n",
      "Learning Curves plot saved and logged.\n",
      "\n",
      "AUC Scores from Cross-Validation (on X_train):\n",
      "Logistic Regression: 0.9535\n",
      "KNears Neighbors: 0.9305\n",
      "Support Vector Classifier: 0.9765\n",
      "Decision Tree Classifier: 0.9320\n",
      "Combined ROC Curve plot saved and logged.\n",
      "Logistic Regression individual ROC Curve plot saved and logged.\n",
      "\n",
      "--- Final Test Set Performance (using original_Xtest/original_ytest) ---\n",
      "\n",
      "--- Logistic Regression Test Set Metrics ---\n",
      "Accuracy Score: 0.9585\n",
      "Precision Score: 0.0432\n",
      "Recall Score: 0.8511\n",
      "F1 Score: 0.0823\n",
      "AUC Score: 0.9311\n",
      "\n",
      "--- K-Nearest Neighbors Test Set Metrics ---\n",
      "Accuracy Score: 0.9906\n",
      "Precision Score: 0.1702\n",
      "Recall Score: 0.8511\n",
      "F1 Score: 0.2837\n",
      "AUC Score: 0.9889\n",
      "\n",
      "--- Support Vector Classifier Test Set Metrics ---\n",
      "Accuracy Score: 0.9963\n",
      "Precision Score: 0.3486\n",
      "Recall Score: 0.8085\n",
      "F1 Score: 0.4872\n",
      "AUC Score: 0.9660\n",
      "\n",
      "--- Decision Tree Classifier Test Set Metrics ---\n",
      "Accuracy Score: 0.9747\n",
      "Precision Score: 0.0707\n",
      "Recall Score: 0.8723\n",
      "F1 Score: 0.1308\n",
      "AUC Score: 0.9443\n",
      "\n",
      "--- Precision-Recall for Logistic Regression on Test Set ---\n",
      "Average precision-recall score: 0.7569\n",
      "Logistic Regression Precision-Recall Curve plot saved and logged.\n",
      "\n",
      "All final test set evaluations and plots have been logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit, learning_curve, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- Start the MLflow Run for Final Test Set Performance and Plots ---\n",
    "with mlflow.start_run(run_name=\"Final_Test_Set_Performance_And_Plots\"):\n",
    "    print(\"Starting MLflow run for final test set evaluation and plots...\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Learning Curves \n",
    "    # ==============================================================================\n",
    "    def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
    "                            n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "        f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "\n",
    "        estimators = [estimator1, estimator2, estimator3, estimator4]\n",
    "        titles = [\"Logistic Regression Learning Curve\", \"K-Nearest Neighbors Learning Curve\",\n",
    "                  \"Support Vector Classifier \\n Learning Curve\", \"Decision Tree Classifier \\n Learning Curve\"]\n",
    "        axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "        for i, estimator in enumerate(estimators):\n",
    "            train_sizes_lc, train_scores, test_scores = learning_curve(\n",
    "                estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "            train_scores_mean = np.mean(train_scores, axis=1)\n",
    "            train_scores_std = np.std(train_scores, axis=1)\n",
    "            test_scores_mean = np.mean(test_scores, axis=1)\n",
    "            test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "            axes[i].fill_between(train_sizes_lc, train_scores_mean - train_scores_std,\n",
    "                                 train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
    "            axes[i].fill_between(train_sizes_lc, test_scores_mean - test_scores_std,\n",
    "                                 test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "            axes[i].plot(train_sizes_lc, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n",
    "            axes[i].plot(train_sizes_lc, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "            axes[i].set_title(titles[i], fontsize=14)\n",
    "            axes[i].set_xlabel('Training size (m)')\n",
    "            axes[i].set_ylabel('Score')\n",
    "            axes[i].grid(True)\n",
    "            axes[i].legend(loc=\"best\")\n",
    "        return plt\n",
    "\n",
    "    cv_lc = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42) # Using a new var name to avoid conflict\n",
    "    learning_curve_plot = plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf,\n",
    "                                              X_train, y_train, (0.87, 1.01), cv=cv_lc, n_jobs=-1) # Use n_jobs=-1 for speed\n",
    "    learning_curve_plot.savefig(\"learning_curves_all_classifiers.png\")\n",
    "    learning_curve_plot.close('all') # Use close('all') to ensure all figures are closed\n",
    "    print(\"Learning Curves plot saved and logged.\")\n",
    "    mlflow.log_artifact(\"learning_curves_all_classifiers.png\")\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Cross-Validation Predictions  - These are on X_train for ROC plots\n",
    "    # ==============================================================================\n",
    "    log_reg_pred_cv = cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\", n_jobs=-1)\n",
    "    knears_pred_cv = cross_val_predict(knears_neighbors, X_train, y_train, cv=5, n_jobs=-1) # KNN doesn't have decision_function\n",
    "    svc_pred_cv = cross_val_predict(svc, X_train, y_train, cv=5, method=\"decision_function\", n_jobs=-1)\n",
    "    tree_pred_cv = cross_val_predict(tree_clf, X_train, y_train, cv=5, n_jobs=-1) # Decision tree doesn't have decision_function\n",
    "    # Note: decision_function is preferred for ROC if available as it uses raw scores.\n",
    "    # For models without it, predict_proba[:,1] could be used, or just predict for binary.\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # AUC Scores - These are on X_train via CV predictions\n",
    "    # ==============================================================================\n",
    "    print('\\nAUC Scores from Cross-Validation (on X_train):')\n",
    "    print('Logistic Regression: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred_cv)))\n",
    "    print('KNears Neighbors: {:.4f}'.format(roc_auc_score(y_train, knears_pred_cv)))\n",
    "    print('Support Vector Classifier: {:.4f}'.format(roc_auc_score(y_train, svc_pred_cv)))\n",
    "    print('Decision Tree Classifier: {:.4f}'.format(roc_auc_score(y_train, tree_pred_cv)))\n",
    "    # Note: These are for training evaluation, not the final test set performance logged below.\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ROC Curve Plotting \n",
    "    # ==============================================================================\n",
    "    log_fpr, log_tpr, log_threshold = roc_curve(y_train, log_reg_pred_cv)\n",
    "    knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred_cv)\n",
    "    svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred_cv)\n",
    "    tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred_cv)\n",
    "\n",
    "    def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.title('ROC Curve \\n Top 4 Classifiers (on CV Predictions)', fontsize=18) # Clarified title\n",
    "        plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred_cv)))\n",
    "        plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred_cv)))\n",
    "        plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred_cv)))\n",
    "        plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred_cv)))\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.axis([-0.01, 1, 0, 1])\n",
    "        plt.xlabel('False Positive Rate', fontsize=16)\n",
    "        plt.ylabel('True Positive Rate', fontsize=16)\n",
    "        plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                     arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                     )\n",
    "        plt.legend()\n",
    "        return plt\n",
    "\n",
    "    combined_roc_plot = graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "    combined_roc_plot.savefig(\"combined_roc_curve_test_set.png\") # Make sure filename is EXACTLY correct\n",
    "    combined_roc_plot.close()\n",
    "    print(\"Combined ROC Curve plot saved and logged.\")\n",
    "    mlflow.log_artifact(\"combined_roc_curve_test_set.png\")\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Logistic Regression Individual ROC Curve \n",
    "    # ==============================================================================\n",
    "    def logistic_roc_curve_plot_func(log_fpr, log_tpr): # Renamed to avoid conflict if any\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.title('Logistic Regression ROC Curve', fontsize=16)\n",
    "        plt.plot(log_fpr, log_tpr, 'b-', linewidth=2)\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlabel('False Positive Rate', fontsize=16)\n",
    "        plt.ylabel('True Positive Rate', fontsize=16)\n",
    "        plt.axis([-0.01,1,0,1])\n",
    "        return plt\n",
    "\n",
    "    log_reg_roc_plot = logistic_roc_curve_plot_func(log_fpr, log_tpr) # Using log_fpr, log_tpr from CV predictions\n",
    "    log_reg_roc_plot.savefig(\"logistic_regression_roc_curve_test_set.png\") # Make sure filename is EXACTLY correct\n",
    "    log_reg_roc_plot.close()\n",
    "    print(\"Logistic Regression individual ROC Curve plot saved and logged.\")\n",
    "    mlflow.log_artifact(\"logistic_regression_roc_curve_test_set.png\")\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Precision-Recall Calculation - still on y_train, log_reg_pred_cv\n",
    "    # ==============================================================================\n",
    "    precision_cv, recall_cv, threshold_cv = precision_recall_curve(y_train, log_reg_pred_cv)\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Final Test Set Performance Metrics Logging\n",
    "    # ==============================================================================\n",
    "    print('\\n--- Final Test Set Performance (using original_Xtest/original_ytest) ---')\n",
    "\n",
    "    classifiers_for_test = {\n",
    "        \"Logistic Regression\": log_reg,\n",
    "        \"K-Nearest Neighbors\": knears_neighbors,\n",
    "        \"Support Vector Classifier\": svc,\n",
    "        \"Decision Tree Classifier\": tree_clf\n",
    "    }\n",
    "\n",
    "    for name, model in classifiers_for_test.items():\n",
    "        print(f\"\\n--- {name} Test Set Metrics ---\")\n",
    "        try:\n",
    "            # For models that have decision_function (Logistic Regression, SVC)\n",
    "            if hasattr(model, \"decision_function\"):\n",
    "                y_score = model.decision_function(original_Xtest)\n",
    "            # For models that have predict_proba (KNeighbors, DecisionTree, also Logistic Regression, SVC)\n",
    "            elif hasattr(model, \"predict_proba\"):\n",
    "                y_score = model.predict_proba(original_Xtest)[:, 1]\n",
    "            else: # Fallback to binary prediction if probabilities/scores not available\n",
    "                y_score = model.predict(original_Xtest) # This will affect AUC/PR curve if not probabilities\n",
    "\n",
    "            y_pred = model.predict(original_Xtest) # Binary predictions for accuracy, precision, recall, f1\n",
    "\n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(original_ytest, y_pred)\n",
    "            prec = precision_score(original_ytest, y_pred)\n",
    "            rec = recall_score(original_ytest, y_pred)\n",
    "            f1 = f1_score(original_ytest, y_pred)\n",
    "            auc_score = roc_auc_score(original_ytest, y_score) # Use y_score for AUC\n",
    "\n",
    "            # Log to MLflow\n",
    "            mlflow.log_metric(f\"{name.replace(' ', '_').lower()}_test_accuracy\", acc)\n",
    "            mlflow.log_metric(f\"{name.replace(' ', '_').lower()}_test_precision\", prec)\n",
    "            mlflow.log_metric(f\"{name.replace(' ', '_').lower()}_test_recall\", rec)\n",
    "            mlflow.log_metric(f\"{name.replace(' ', '_').lower()}_test_f1_score\", f1)\n",
    "            mlflow.log_metric(f\"{name.replace(' ', '_').lower()}_test_auc_score\", auc_score)\n",
    "\n",
    "            # Print to console\n",
    "            print(f\"Accuracy Score: {acc:.4f}\")\n",
    "            print(f\"Precision Score: {prec:.4f}\")\n",
    "            print(f\"Recall Score: {rec:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate metrics for {name}: {e}\")\n",
    "            mlflow.log_param(f\"{name.replace(' ', '_').lower()}_test_error\", str(e))\n",
    "\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Precision-Recall Curve \n",
    "    # This specifically uses Logistic Regression's decision_function on original_Xtest\n",
    "    # ==============================================================================\n",
    "    print('\\n--- Precision-Recall for Logistic Regression on Test Set ---')\n",
    "    try:\n",
    "        # Assuming log_reg has decision_function\n",
    "        undersample_y_score = log_reg.decision_function(original_Xtest)\n",
    "        undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n",
    "\n",
    "        print('Average precision-recall score: {0:0.4f}'.format(undersample_average_precision))\n",
    "        mlflow.log_metric(\"log_reg_test_average_precision_score\", undersample_average_precision)\n",
    "\n",
    "        fig_pr, ax_pr = plt.figure(figsize=(12,6)), plt.gca() # Get current axes to control plot\n",
    "        precision_pr, recall_pr, _ = precision_recall_curve(original_ytest, undersample_y_score)\n",
    "\n",
    "        plt.step(recall_pr, precision_pr, color='#004a93', alpha=0.2, where='post')\n",
    "        plt.fill_between(recall_pr, precision_pr, step='post', alpha=0.2, color='#48a6ff')\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.title('UnderSampling Precision-Recall curve (Logistic Regression): \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
    "                  undersample_average_precision), fontsize=16) # Clarified title\n",
    "        plt.savefig(\"logistic_regression_precision_recall_curve_test_set.png\") # Make sure filename is EXACTLY correct\n",
    "        plt.close(fig_pr) # Close the specific figure\n",
    "\n",
    "        print(\"Logistic Regression Precision-Recall Curve plot saved and logged.\")\n",
    "        mlflow.log_artifact(\"logistic_regression_precision_recall_curve_test_set.png\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate Precision-Recall curve for Logistic Regression: {e}\")\n",
    "        mlflow.log_param(\"log_reg_pr_curve_error\", str(e))\n",
    "\n",
    "    print(\"\\nAll final test set evaluations and plots have been logged to MLflow.\")\n",
    "\n",
    "# End of the MLflow run context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f56e6936-314c-42d4-8ea2-0cb2386ad382",
    "_uuid": "d6e62d64e9d9aa70223576a1df91a008aa6c2664"
   },
   "source": [
    "## A Deeper Look into LogisticRegression:\n",
    "<a id=\"logistic\"></a>\n",
    "In this section we will ive a deeper look into the <b> logistic regression classifier</b>.\n",
    "\n",
    "\n",
    "### Terms:\n",
    "<ul>\n",
    "<li><b>True Positives:</b> Correctly Classified Fraud Transactions </li>\n",
    "<li><b>False Positives:</b> Incorrectly Classified Fraud Transactions</li>\n",
    "<li> <b>True Negative:</b> Correctly Classified Non-Fraud Transactions</li>\n",
    "<li> <b>False Negative:</b> Incorrectly Classified Non-Fraud Transactions</li>\n",
    "<li><b>Precision: </b>  True Positives/(True Positives + False Positives)  </li>\n",
    "<li><b> Recall: </b> True Positives/(True Positives + False Negatives)   </li>\n",
    "<li> Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.</li>\n",
    "<li><b>Precision/Recall Tradeoff: </b> The more precise (selective) our model is, the less cases it will detect. Example: Assuming that our model has a precision of 95%, Let's say there are only 5 fraud cases in which the model is 95% precise or more that these are fraud cases. Then let's say there are 5 more cases that our model considers 90% to be a fraud case, if we lower the precision there are more cases that our model will be able to detect. </li>\n",
    "</ul>\n",
    "\n",
    "### Summary:\n",
    "<ul>\n",
    "<li> <b>Precision starts to descend</b> between 0.90 and 0.92 nevertheless, our precision score is still pretty high and still we have a decent recall score. </li>\n",
    "\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcpcrit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
